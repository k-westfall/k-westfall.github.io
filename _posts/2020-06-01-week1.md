---
layout: post
title: Week 1
---
# Weekly Check-In
The Secure and Transparent Systems Laboratory welcomes us to the lab. Andy and I meet Professor Bates and two of the lab's PhD students that are working this summer, Jason Liu and Adil Inam.

The lab meeting consists of introduction to current projects of the lab and follows with Jason practicing for an upcoming presentation at the IEEE European Symposium on Security and Privacy.

# Current Tasks
* Get set up on lab server with virtual machines for development work.
* Get access to lab datasets. In particular make sure we can download and access the OpenStack dataset and StreamSpot dataset.
* Goal for the week is to get the DeepLog model for anomaly detection setup and running and train it on a portion of the StreamSpot data.

Summary of progress
We committed the first part of the week to getting all of our systems set up on the lab servers. This meant getting virtual machines set up for development, getting access to the labâ€™s shared project development server, and getting access to lab datasets that we will use going forward.

Our next task was to start gaining familiarity with some of the tools being used in current lab projects. Primarily this meant getting the DeepLog model running on the OpenStack and StreamSpot data. The OpenStack dataset contains system log events that contain normal (benign) user activity as well as log data from machines with known malicious behavior. The StreamSpot data is a much larger dataset that processes system log data into a graph format for analysis.

The goal of training DeepLog on the OpenStack data was to make sure we had implemented the model correctly, and compare our results to exist runs on this dataset. After successfully replicating the results on OpenStack, we our goal was to run DeepLog on StreamSpot, which was a new application of the DeepLog model.

We ran in to some challenges trying to adapt the DeepLog model to the StreamSpot data. The StreamSpot data has a substantially different structure than OpenStack, in that OpenStack consists of logging data from a large number of systems. The StreamSpot data on the other hand, provides a provenance graph representation of all events on the system. (A provenance graph for system data represents system objects such as files as nodes, while system processes are represented as edges in the graph.)

The other problem with working with the StreamSpot data was dealing with the size of the dataset. This necessitated finding effective ways to scale down the size of the dataset we were training on without introducing a bias into the training dataset.
